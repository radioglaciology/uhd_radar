{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "client = Client(n_workers=1,\n",
    "                threads_per_worker=6,\n",
    "                memory_limit='10GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "import time\n",
    "import os\n",
    "\n",
    "import dask\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import hvplot.xarray\n",
    "import holoviews as hv\n",
    "import scipy.constants\n",
    "import scipy\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "import processing_dask as pr\n",
    "import plot_dask\n",
    "\n",
    "sys.path.append(\"../../../preprocessing/\")\n",
    "from generate_chirp import generate_chirp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams.update({\n",
    "        'font.size': 16,\n",
    "        'legend.fontsize': 10,\n",
    "        'lines.linewidth': 2,\n",
    "        'text.usetex': False\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prefix = \"/media/thomas/Extreme SSD/orca_paper_data_files/phase_noise/b205/20240222_203345\"\n",
    "\n",
    "prefix = \"/home/thomas/Documents/StanfordGrad/RadioGlaciology/drone/radar_data/orca_paper_data_files/phase_noise/b205/20240222_203345\"\n",
    "#prefix = \"/media/thomas/Extreme SSD/orca_paper_data_files/phase_noise/b205/heat_experiment/20240301_003904\" # heat experiment\n",
    "#prefix = \"/media/thomas/Extreme SSD/orca_paper_data_files/phase_noise/b205/20240305_193939\" # replication of original\n",
    "#prefix = \"/media/thomas/Extreme SSD/orca_paper_data_files/phase_noise/b205/20240306_155551\" # 10 dB higher TX power, 10 dB lower RX gain\n",
    "#prefix = \"/media/thomas/Extreme SSD/orca_paper_data_files/phase_noise/b205/20240306_183951\" # 30 dB attenuator switched to RX side\n",
    "#prefix = \"/media/thomas/Extreme SSD/orca_paper_data_files/phase_noise/b205/20240306_192829\" # 30 dB attenuator switched back to TX side (back to orig config)\n",
    "#prefix = \"/media/thomas/Extreme SSD/orca_paper_data_files/phase_noise/b205/20240306_210308\" # fiber\n",
    "\n",
    "\n",
    "zero_sample_idx = 159\n",
    "sig_speed = scipy.constants.speed_of_light * (2/3)\n",
    "\n",
    "zarr_base_location=\"/home/thomas/Documents/StanfordGrad/RadioGlaciology/test_tmp_zarr_cache/\"\n",
    "zarr_path = pr.save_radar_data_to_zarr(prefix, zarr_base_location=zarr_base_location, skip_if_cached=True)\n",
    "\n",
    "zarr_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = xr.open_zarr(zarr_path)\n",
    "#raw = raw[{'pulse_idx': slice(0, 10000)}]\n",
    "if 'NOTES' in raw.attrs['config']:\n",
    "    print(\"=== Notes from config file: ===\")\n",
    "    print(raw.attrs['config']['NOTES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chirp_ts, chirp = generate_chirp(raw.config)\n",
    "\n",
    "compressed = pr.pulse_compress(raw, chirp,\n",
    "                               fs=raw.config['GENERATE']['sample_rate'],\n",
    "                               zero_sample_idx=zero_sample_idx,\n",
    "                               signal_speed=sig_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_zarr_path = os.path.join(zarr_base_location, raw.basename+\"_pulsecompressed.zarr\")\n",
    "\n",
    "# COMMENT THIS OUT IF THE FILE IS ALREADY GENERATED\n",
    "#print(\"Generating and writing pulse compressed data to: \", compressed_zarr_path)\n",
    "#compressed.to_zarr(compressed_zarr_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressed data\n",
    "\n",
    "Comrpessed data is now saved to disk and we can load it from there. Optionally\n",
    "use some of these plots to verify the peak index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now re-open \"compressed\" but directly from the zarr file\n",
    "compressed = xr.open_dataset(compressed_zarr_path, chunks={\"pulse_idx\": 1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_pwr = xr.apply_ufunc(lambda x: np.abs(x)**2, compressed[\"radar_data\"], dask=\"parallelized\", output_dtypes=[np.float32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relatively fast approach, but only suitable if the approximate peak is known and we're just verifying\n",
    "# Plot every millionth pulse compressed data and zoom in around the expected peak index\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(1, 7):\n",
    "    ax.plot(compressed_pwr[{'pulse_idx': (i*1000000)-1}], label=f\"pulse_idx={i*1000000-1}\")\n",
    "ax.set_xlim(185, 190)\n",
    "#ax.set_xlim(225, 232)\n",
    "#ax.set_xlim(155, 160)\n",
    "ax.grid()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More complete (but very slow) way to check peak index\n",
    "The cells below will find the peak around an approximate distance in every pulse compressed chirp. This is slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected_reflector_distance_1way = 50 # m\n",
    "# reflector_peak_tol_bins = 2 # bins (on each side)\n",
    "# noise_start_distance_1way = 1000 # m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected_peak_idx = (np.abs(compressed.reflection_distance - expected_reflector_distance_1way)).argmin().compute().item()\n",
    "\n",
    "# peak_idxs = compressed[\"radar_data\"].reduce(\n",
    "#     lambda x, axis: (np.abs((x[:, expected_peak_idx-reflector_peak_tol_bins:expected_peak_idx+reflector_peak_tol_bins]))\n",
    "#                         ).argmax(axis=axis) + expected_peak_idx-reflector_peak_tol_bins, dim='travel_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Somehow figure out the peak_idx (either take median of peak_idxs or guess and verify with the millionth sample plot)\n",
    "peak_idx = 187\n",
    "#peak_idx = 229\n",
    "#peak_idx = 159"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and save just the peak from each chirp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "compressed_single_peak = xr.apply_ufunc(\n",
    "    lambda x: x[peak_idx],\n",
    "    compressed[\"radar_data\"],\n",
    "    input_core_dims=[['travel_time']], # The dimension operated over -- aka \"don't vectorize over this\"\n",
    "    output_core_dims=[[]], # The output dimensions of the lambda function itself\n",
    "    exclude_dims=set((\"travel_time\",)), # Dimensions to not vectorize over\n",
    "    vectorize=True, # Vectorize other dimensions using a call to np.vectorize\n",
    "    dask=\"parallelized\", # Allow dask to chunk and parallelize the computation\n",
    "    output_dtypes=[np.complex64], # Needed for dask: explicitly provide the output dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_single_peak_zarr_path = compressed_zarr_path.replace(\".zarr\", f\"_single_peak{peak_idx}.zarr\")\n",
    "print(compressed_single_peak_zarr_path)\n",
    "\n",
    "# COMMENT THIS OUT IF THE FILE IS ALREADY GENERATED\n",
    "compressed_single_peak.chunk('auto').to_zarr(compressed_single_peak_zarr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now re-open compressed_single_peak from the zarr file\n",
    "\n",
    "compressed_single_peak = xr.open_zarr(compressed_single_peak_zarr_path)[\"radar_data\"]\n",
    "\n",
    "# USE ONLY FOR PHASE CORRECTION TEST\n",
    "#compressed_single_peak = compressed_single_peak_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute signal statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.logspace(np.log10(250e-6), np.log10(2.5*60), 10)\n",
    "actual_stack_t = np.nan * np.zeros_like(ts)\n",
    "actual_stack_n = np.zeros_like(ts, dtype=int)\n",
    "\n",
    "# Statistics to compute\n",
    "stack_signal_peak_pwr_mean = np.nan * np.zeros_like(ts)\n",
    "stack_signal_peak_pwr_variance = np.nan * np.zeros_like(ts)\n",
    "stack_signal_peak_phase = np.nan * np.zeros_like(ts)\n",
    "stack_signal_peak_phase_variance = np.nan * np.zeros_like(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_idx, t in enumerate(ts):\n",
    "    if not np.isnan(stack_signal_peak_phase_variance[t_idx]):\n",
    "        continue # Skip if already computed (in case of interruption and restart)\n",
    "    \n",
    "    timestamp = time.time() # Track computation time \n",
    "\n",
    "    actual_stack_n[t_idx] = max(1, int(t / raw.attrs['config']['CHIRP']['pulse_rep_int']))\n",
    "    actual_stack_t[t_idx] = actual_stack_n[t_idx] * raw.attrs['config']['CHIRP']['pulse_rep_int'] # TODO: Account for errors?\n",
    "    print(f\"[{t_idx+1}/{len(ts)}] \\tt={actual_stack_t[t_idx]} \\tn_stack={actual_stack_n[t_idx]}\")\n",
    "    \n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "\n",
    "        if actual_stack_n[t_idx] == 1:\n",
    "            # No need to stack, just compute the statistics\n",
    "            stacked = compressed_single_peak\n",
    "        else:\n",
    "            n_stacks_expected = compressed_single_peak.pulse_idx.size // actual_stack_n[t_idx]\n",
    "            stacked = pr.stack(compressed_single_peak, actual_stack_n[t_idx])\n",
    "    \n",
    "        peak_phases_numpy = np.angle(stacked)\n",
    "        peak_pwr = (np.abs(stacked))**2\n",
    "\n",
    "        stack_signal_peak_pwr_mean[t_idx] = peak_pwr.mean().compute().item()\n",
    "        stack_signal_peak_pwr_variance[t_idx] = peak_pwr.var().compute().item()\n",
    "        stack_signal_peak_phase[t_idx] = peak_phases_numpy.mean()\n",
    "        stack_signal_peak_phase_variance[t_idx] = peak_phases_numpy.var()\n",
    "        \n",
    "    \n",
    "    print(f\"Completed in {time.time() - timestamp} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summarized results to a file\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "filename = f\"outputs/{raw.basename}_stacking_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}_bin{peak_idx}.pickle\"\n",
    "\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        \"basename\": raw.basename,\n",
    "        \"actual_stack_t\": actual_stack_t,\n",
    "        \"actual_stack_n\": actual_stack_n,\n",
    "        \"stack_signal_peak_pwr_mean\": stack_signal_peak_pwr_mean,\n",
    "        \"stack_signal_peak_pwr_variance\": stack_signal_peak_pwr_variance,\n",
    "        \"stack_signal_peak_phase\": stack_signal_peak_phase,\n",
    "        \"stack_signal_peak_phase_variance\": stack_signal_peak_phase_variance\n",
    "    }, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "\n",
    "with open(filename, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
    "ax_pwr, ax_ph = axs\n",
    "\n",
    "ax_pwr.plot(data[\"actual_stack_t\"], data[\"stack_signal_peak_pwr_mean\"])\n",
    "# Add a shaded region for the variance\n",
    "ax_pwr.fill_between(data[\"actual_stack_t\"], data[\"stack_signal_peak_pwr_mean\"] - np.sqrt(data[\"stack_signal_peak_pwr_variance\"]),\n",
    "                    data[\"stack_signal_peak_pwr_mean\"] + np.sqrt(data[\"stack_signal_peak_pwr_variance\"]), alpha=0.4)\n",
    "ax_pwr.set_title(f\"Signal Power [{data['basename']}]\")\n",
    "ax_pwr.loglog()\n",
    "\n",
    "ph = np.degrees(data[\"stack_signal_peak_phase\"])\n",
    "ph_var = np.degrees(data[\"stack_signal_peak_phase_variance\"])\n",
    "\n",
    "# ax_ph.plot(data[\"actual_stack_t\"], ph)\n",
    "# # Add a shaded region for the variance\n",
    "# ax_ph.fill_between(data[\"actual_stack_t\"], ph - np.sqrt(ph_var),\n",
    "#                     ph + np.sqrt(ph_var), alpha=0.4)\n",
    "\n",
    "ax_ph.plot(data[\"actual_stack_t\"], (data[\"stack_signal_peak_phase_variance\"]))\n",
    "ax_ph.set_title(\"Variance of Signal Phase\")\n",
    "ax_ph.loglog()\n",
    "ax_ph.set_xlabel(\"Wall Clock Integration Time [s]\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.grid(True)\n",
    "\n",
    "plot_filename = filename.replace(\".pickle\", \"_plot.png\")\n",
    "fig.savefig(plot_filename)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression on single-pulse phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = (np.angle(compressed_single_peak))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_var_deg_theory = (1/10**(20/10)) * (180/np.pi)**2\n",
    "\n",
    "phase_var_deg_emp = np.var(np.degrees(phases))\n",
    "print(f\"Empirical phase variance: {phase_var_deg_emp} deg\")\n",
    "print(f\"Theoretical phase variance: {phase_var_deg_theory} deg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "ax.scatter(compressed_single_peak.slow_time/60, np.degrees(phases), s=0.01, alpha=0.4)\n",
    "\n",
    "# Add linear regression line\n",
    "from scipy.stats import linregress\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = linregress(compressed_single_peak.slow_time, np.degrees(phases))\n",
    "ax.plot(compressed_single_peak.slow_time/60, slope*compressed_single_peak.slow_time + intercept, color='C3', linestyle='--', label='Linear Regression Fit', linewidth=1)\n",
    "print(f\"Linear regression fit: slope={slope} degrees/second, r_value={r_value}, p_value={p_value}, std_err={std_err}\")\n",
    "ax.text(0.05, 0.93, f\"Linear regression slope: {slope:.6f} degrees/second\", transform=ax.transAxes, fontsize=10)\n",
    "\n",
    "ax.annotate(text='', xy=(5,np.mean(np.degrees(phases))-(phase_var_deg_emp/2)), xytext=(5,np.mean(np.degrees(phases))+(phase_var_deg_emp/2)), arrowprops=dict(arrowstyle='<->', color='C1', linewidth=2), color='C1')\n",
    "ax.annotate(text=f'Var$(\\phi)={phase_var_deg_emp:.2f}$ degrees', xy=(5.5, -130), color='C1')\n",
    "\n",
    "ax.set_xlabel('Slow Time [minutes]')\n",
    "ax.set_ylabel('Phase [degrees]')\n",
    "#ax.set_title(f\"{raw.basename}\")\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(plot_filename.replace(\".png\", \"_phase_linreg.png\"), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase correction\n",
    "\n",
    "Use this to first estimate a phase correction term, then apply the phase correction,\n",
    "then re-run the last section of cells with `compressed_single_peak_corrected` instead of `compressed_single_peak`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_correction = np.exp(-1j*np.radians(slope * compressed_single_peak.slow_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_single_peak_corrected = compressed_single_peak * phase_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
