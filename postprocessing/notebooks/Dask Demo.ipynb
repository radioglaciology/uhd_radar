{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview of dask-backed radar processing interface\n",
    "\n",
    "Most of the core code is packaged into `processing_dask.py`, so that file is worth a browse.\n",
    "\n",
    "In the immediate future, the core advantage is dask's ability to automatically chunk data such\n",
    "that small chunks of giant files can be processed without running out of memory. To facilitate this,\n",
    "you can set a memory limit in the next cell. Dask will process chunks until it runs against this\n",
    "limit and then start storing already-completed results to disk.\n",
    "\n",
    "In the longer term, this infrastructure should also allow us to do some other fun tricks, such as\n",
    "storing out data in cloud storage buckets and automatically farming out computations to\n",
    "SLURM-managed clusters (i.e. Sherlock) or cloud compute services.\n",
    "\n",
    "There are some new dependencies:\n",
    "* xarray\n",
    "* dask\n",
    "* zarr\n",
    "* hvplot\n",
    "* datashader\n",
    "\n",
    "The last two are technically only needed for plotting (more about that below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This setup activates the \"distributed\" scheduler. In this case, we're still running it locally, but this gives us access to additional features (i.e. memory limits).\n",
    "# See https://docs.dask.org/en/stable/scheduling.html\n",
    "from dask.distributed import Client, LocalCluster\n",
    "client = Client(n_workers=4,\n",
    "                threads_per_worker=1,\n",
    "                memory_limit='3GB') # Note that `memory_limit` is the limit **per worker**.\n",
    "client # If you click the dashboard link in the output, you can monitor real-time progress and get other cool visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import hvplot.xarray\n",
    "import scipy.constants\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import processing_dask as pr\n",
    "import plot_dask\n",
    "import processing as old_processing\n",
    "\n",
    "sys.path.append(\"../../preprocessing/\")\n",
    "from generate_chirp import generate_chirp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prefix = \"/home/thomas/Documents/StanfordGrad/RadioGlaciology/drone/radar_data/20230621-bench-prf-testing/20230621_163428\"\n",
    "prefix = \"/home/thomas/Documents/StanfordGrad/RadioGlaciology/drone/radar_data/20230621-bench-prf/20230622_104230\"\n",
    "#prefix = \"/home/thomas/Documents/StanfordGrad/RadioGlaciology/drone/radar_data/20230621-bench-prf/20230621_173826\"\n",
    "\n",
    "zero_sample_idx = 159\n",
    "\n",
    "zarr_path = pr.save_radar_data_to_zarr(prefix, zarr_base_location=\"/home/thomas/Documents/StanfordGrad/RadioGlaciology/test_tmp_zarr_cache/\", skip_if_cached=False)\n",
    "\n",
    "zarr_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = xr.open_zarr(zarr_path)\n",
    "\n",
    "raw # Uncomment this to explore the structure of the resulting dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're new to xarray, it's probably worth a read through some of their intro docs: https://docs.xarray.dev/en/stable/getting-started-guide/quick-overview.html\n",
    "\n",
    "The dataset has a single \"data variable\" called `radar_data` (and I think we should keep it this way).\n",
    "\n",
    "In addition to the data variable, there are dimensions, coordinates, and attributes.\n",
    "\n",
    "Dimensions are just names for the indices into the data variables. You can see them above under \"indexes\". For us, they are `pulse_idx` and `sample_idx`.\n",
    "\n",
    "Coordinates are variables associated to one or more dimensions that are sort of like metadata. In our case, we have `pulse_idx`, `slow_time`, `sample_idx`, and `fast_time`. Coordinates that have the same name as a dimension are considered \"dimension coordinates\" and can be directly used for indexing. You can only have one dimension coordinate per dimension, but you can easily swap which coordinate is the dimension coordinate with `swap_dims()` (see, for example, the second plotting example).\n",
    "\n",
    "More about this here: https://docs.xarray.dev/en/stable/user-guide/data-structures.html#coordinates\n",
    "\n",
    "Attributes serves as a dictionary of random other stuff you can package with your dataset. I've put the config and log output there. For example, you can access anything from the associated config YAML like this:\n",
    "\n",
    "`raw.attrs['config']['GENERATE']['sample_rate']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = pr.remove_errors(raw)\n",
    "stacked = pr.stack(stacked,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_blackman_window = copy.deepcopy(stacked.config)\n",
    "config_blackman_window['GENERATE']['window'] = 'blackman'\n",
    "\n",
    "chirp_ts, chirp = generate_chirp(config_blackman_window)\n",
    "compressed = pr.pulse_compress(stacked, chirp,\n",
    "                               fs=stacked.config['GENERATE']['sample_rate'],\n",
    "                               zero_sample_idx=zero_sample_idx,\n",
    "                               signal_speed=scipy.constants.c * (2/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything has run quickly up to now because (apart from the data translation to zarr) no actual data has been processed.\n",
    "\n",
    "\n",
    "When you actually request data (by calling `.compute()` on it), the processing actually begins. Plotting triggers this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This roughly matches the old plot_radargram function I was using and uses matplotlib\n",
    "plot_dask.plot_radargram(compressed, sig_speed=scipy.constants.c * (2/3), figsize=(10,6), vmin=-90, vmax=-40, ylims=(100, -50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's an example of a parallelizable implementation of log compression\n",
    "return_power = xr.apply_ufunc(\n",
    "    lambda x: 20*np.log10(np.abs(x)),\n",
    "    compressed,\n",
    "    dask=\"parallelized\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you expect to repeatedly need the same data, you can explicitly tell dask to keep it around in memory.\n",
    "#return_power.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of plotting using hvplot -- this is supposed to be more efficient at directly plotting giant datasets\n",
    "# I haven't really dug into this much yet, but there's documentation here: https://holoviews.org/user_guide/Large_Data.html\n",
    "# In the backend, this uses bokeh, so the plots are also interactive\n",
    "return_power.swap_dims({'pulse_idx': 'slow_time', 'travel_time': 'reflection_distance'}).hvplot.quadmesh(x='slow_time', ylim=(100,-50), clim=(-90,-40), cmap='inferno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rg2]",
   "language": "python",
   "name": "conda-env-rg2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb81f79795c75689c11bb9ecc505fc7b83ca5f9665fb7ad4bb0ce31dc1de5ece"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
